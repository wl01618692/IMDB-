{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4.ipynb","provenance":[],"authorship_tag":"ABX9TyPGR8VW2pvWuwTVkHP2o9n9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"Zn0-z5pVA-63","executionInfo":{"status":"ok","timestamp":1607030385531,"user_tz":480,"elapsed":1306,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"3b2748a4-f82a-4f03-dd03-294f893fa599"},"source":["# Read url\n","import numpy\n","import pandas as pd\n","df = pd.read_csv('https://raw.githubusercontent.com/lkyin/ECS189L/main/Tweets.csv')\n","\n","# Preview\n","df.head()"],"execution_count":155,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>airline_sentiment</th>\n","      <th>airline_sentiment_confidence</th>\n","      <th>negativereason</th>\n","      <th>negativereason_confidence</th>\n","      <th>airline</th>\n","      <th>airline_sentiment_gold</th>\n","      <th>name</th>\n","      <th>negativereason_gold</th>\n","      <th>retweet_count</th>\n","      <th>text</th>\n","      <th>tweet_coord</th>\n","      <th>tweet_created</th>\n","      <th>tweet_location</th>\n","      <th>user_timezone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>570306133677760513</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>cairdin</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica What @dhepburn said.</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:35:52 -0800</td>\n","      <td>NaN</td>\n","      <td>Eastern Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>570301130888122368</td>\n","      <td>positive</td>\n","      <td>0.3486</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica plus you've added commercials t...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:59 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>570301083672813571</td>\n","      <td>neutral</td>\n","      <td>0.6837</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>yvonnalynn</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:48 -0800</td>\n","      <td>Lets Play</td>\n","      <td>Central Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>570301031407624196</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Bad Flight</td>\n","      <td>0.7033</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica it's really aggressive to blast...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:36 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>570300817074462722</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Can't Tell</td>\n","      <td>1.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica and it's a really big bad thing...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:14:45 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             tweet_id  ...               user_timezone\n","0  570306133677760513  ...  Eastern Time (US & Canada)\n","1  570301130888122368  ...  Pacific Time (US & Canada)\n","2  570301083672813571  ...  Central Time (US & Canada)\n","3  570301031407624196  ...  Pacific Time (US & Canada)\n","4  570300817074462722  ...  Pacific Time (US & Canada)\n","\n","[5 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":155}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-vOkcxrxZsa","executionInfo":{"status":"ok","timestamp":1607030385531,"user_tz":480,"elapsed":1294,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"2b6a90fb-61e2-4cad-ed47-25dffa4b31dc"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","print('Done!')"],"execution_count":156,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sulkc6keJi1s"},"source":["## ***Q1***\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7Lii-iFguM_","executionInfo":{"status":"ok","timestamp":1607030391919,"user_tz":480,"elapsed":7677,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"b4de3c3d-30b0-4c0e-bc21-86490c65e928"},"source":["# Q1\n","## initialization\n","Q1_df = df\n","airline_sentiment = df['airline_sentiment']\n","\n","## Store airline names into an array\n","airline_names = df[\"airline\"].unique()\n","\n","## Remove punctuation\n","Q1_df['text'] = Q1_df['text'].str.replace(\"!\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\".\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"#\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\",\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\":\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\")\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"(\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"?\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"-\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"I\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"'\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"&\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\";\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"|\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('”', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('“', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('``', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('\"', \"\")\n","\n","# Stemming\n","from nltk.stem import PorterStemmer\n","ps = PorterStemmer()\n","for index, value in Q1_df['text'].items():\n","  # Lower Case\n","  value = ps.stem(value.lower())\n","  Q1_df['text'].iloc[index] = value\n","\n","## Remove @airline\n","for values in airline_names:\n","  # Add @ sign before airline names\n","  values = \"@\" + values\n","  # Remove empty space\n","  values = values.replace(\" \",\"\")\n","  if (values == \"@American\"):\n","    values = \"@AmericanAir\"\n","  elif (values == \"@Southwest\"):\n","    values = \"@SouthwestAir\"\n","  # Replace\n","  Q1_df['text'] = Q1_df['text'].str.replace(values, \"\")\n","\n","Q1_df['text'] = Q1_df['text'].str.replace(\"@USairways\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"@UsAirways\", \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"@JetBlue\", \"\")\n","\n","## Remove all lower case @airline\n","Q1_df['text'] = Q1_df['text'].str.replace('@united', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('@jetblue', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('@southwestair', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('@americanair', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('@usairways', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace('@virginamerica', \"\")\n","Q1_df['text'] = Q1_df['text'].str.replace(\"@\", \"\")\n","\n","# Preview\n","print(\"\\n\")\n","Q1_df['text'].head()"],"execution_count":157,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0                                   what dhepburn said\n","1     plus youve added commercials to the experienc...\n","2      didnt today must mean  need to take another ...\n","3     its really aggressive to blast obnoxious ente...\n","4              and its a really big bad thing about it\n","Name: text, dtype: object"]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veUeXv35OUJK","executionInfo":{"status":"ok","timestamp":1607030392421,"user_tz":480,"elapsed":8174,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"01b4de4a-42f0-44ee-f332-fbcb85ba1433"},"source":["## Split into three sentiment groups: positive, neutral, negative\n","Q1_positive = df[airline_sentiment == 'positive']\n","Q1_neutral = df[airline_sentiment == 'neutral']\n","Q1_negative = df[airline_sentiment == 'negative']\n","\n","## Positive sentiments\n","## Remove stopwords\n","wordlist = []\n","for index, value in Q1_positive['text'].items():\n","  stop_words = set(stopwords.words('english'))\n","  ## Tokenizing\n","  word_tokens = word_tokenize(value)\n","  for w in word_tokens:\n","     if not w in stop_words:\n","       wordlist.append(w)\n","\n","## Report\n","from collections import Counter\n","print(\"Top 10 words used in positive sentiment group are:\")\n","print(Counter(wordlist).most_common(10))"],"execution_count":158,"outputs":[{"output_type":"stream","text":["Top 10 words used in positive sentiment group are:\n","[('thank', 594), ('thanks', 468), ('flight', 375), ('great', 234), ('service', 134), ('love', 130), ('get', 114), ('much', 109), ('customer', 108), ('good', 107)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QjIWpf1OXKr","executionInfo":{"status":"ok","timestamp":1607030393442,"user_tz":480,"elapsed":9190,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"e3bc6b4d-ef32-47fa-9055-7454001a08f4"},"source":["## Neutral sentiments\n","## Remove stopwords\n","wordlist = []\n","for index, value in Q1_neutral['text'].items():\n","  stop_words = set(stopwords.words('english'))\n","  ## Tokenizing\n","  word_tokens = word_tokenize(value)\n","  for w in word_tokens:\n","     if not w in stop_words:\n","       wordlist.append(w)\n","\n","## Report\n","print(\"Top 10 words used in neutral sentiment group are:\")\n","print(Counter(wordlist).most_common(10))"],"execution_count":159,"outputs":[{"output_type":"stream","text":["Top 10 words used in neutral sentiment group are:\n","[('flight', 620), ('get', 238), ('need', 163), ('help', 162), ('please', 154), ('flights', 150), ('thank', 148), ('dm', 123), ('would', 122), ('fleek', 107)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_OcCNXOOaMF","executionInfo":{"status":"ok","timestamp":1607030396732,"user_tz":480,"elapsed":12476,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"44f610c3-8a33-4efb-a67e-cd6c3f2096d9"},"source":["## Negative sentiments\n","## Remove stopwords\n","wordlist = []\n","for index, value in Q1_negative['text'].items():\n","  stop_words = set(stopwords.words('english'))\n","  ## Tokenizing\n","  word_tokens = word_tokenize(value)\n","  for w in word_tokens:\n","     if w not in stop_words:\n","       wordlist.append(w)\n","\n","## Report\n","print(\"Top 10 words used in negative sentiment group are:\")\n","print(Counter(wordlist).most_common(10))"],"execution_count":160,"outputs":[{"output_type":"stream","text":["Top 10 words used in negative sentiment group are:\n","[('flight', 2939), ('get', 982), ('cancelled', 920), ('service', 636), ('help', 627), ('hold', 611), ('hours', 600), ('customer', 590), ('2', 534), ('time', 511)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x1OPvMNQJsqL"},"source":["## ***Q2***"]},{"cell_type":"code","metadata":{"id":"XKKfQ3BwnhGO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607030397240,"user_tz":480,"elapsed":12980,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"fca82a5d-2b3c-498b-844d-547a8f309821"},"source":["# Q2\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","## Predictors and output\n","X = Q1_df['text']\n","Y = Q1_df['airline_sentiment']\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(X)\n","\n","## Split dataset into 80% for training and 20% for testing\n","## stratify parameter ensures each airline is represented in training and testing data\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify = Y, random_state = 23)\n","\n","## Train\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","print('Done Training')\n","\n","## Predict\n","y_pred = model.predict(X_test)\n","report = classification_report(y_test, y_pred, output_dict = True)\n","print(pd.DataFrame(report).transpose())\n","\n","## Cross Validation for not overfitting\n","scores = cross_val_score(model, X, Y, cv=10)"],"execution_count":161,"outputs":[{"output_type":"stream","text":["Done Training\n","              precision    recall  f1-score      support\n","negative       0.662079  0.995643  0.795301  1836.000000\n","neutral        0.841121  0.145161  0.247593   620.000000\n","positive       0.833333  0.105932  0.187970   472.000000\n","accuracy       0.672131  0.672131  0.672131     0.672131\n","macro avg      0.778845  0.415579  0.410288  2928.000000\n","weighted avg   0.727598  0.672131  0.581422  2928.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFVAbHihdcrY","executionInfo":{"status":"ok","timestamp":1607030397241,"user_tz":480,"elapsed":12977,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"2f00f71b-231b-4285-a2ba-71a043a8b131"},"source":["## Report\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","print('Accuracy score for this model is: %0.3f' % accuracy_score(y_test, y_pred))\n","print(\"Accuracy score for Cross Validation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":162,"outputs":[{"output_type":"stream","text":["Accuracy score for this model is: 0.672\n","Accuracy score for Cross Validation: 0.66 (+/- 0.02)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ftLnwtlzJuRI"},"source":["## ***Q3***\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOEvxm-QnhIx","executionInfo":{"status":"ok","timestamp":1607030397241,"user_tz":480,"elapsed":12972,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"2165b602-5a61-4ddc-814f-634bb9d819d3"},"source":["# Q3\n","# Assuming fraction over number of sentiments specific to each airline\n","airline_names = df[\"airline\"].unique()\n","\n","# Frequency of positive, neutral, and negative sentiment for each airline\n","neg = {\"Virgin America\": 0 ,\"United\" : 0, \"Southwest\":0 , \"Delta\": 0, \"US Airways\" : 0, \"American\" : 0}\n","neutral = {\"Virgin America\": 0 ,\"United\" : 0, \"Southwest\":0 , \"Delta\": 0, \"US Airways\" : 0, \"American\" : 0}\n","pos = {\"Virgin America\": 0 ,\"United\" : 0, \"Southwest\":0 , \"Delta\": 0, \"US Airways\" : 0, \"American\" : 0}\n","\n","# Fraction of positives and negatives for each airline\n","neg_frac = {\"Virgin America\": 0 ,\"United\" : 0, \"Southwest\":0 , \"Delta\": 0, \"US Airways\" : 0, \"American\" : 0}\n","pos_frac = {\"Virgin America\": 0 ,\"United\" : 0, \"Southwest\":0 , \"Delta\": 0, \"US Airways\" : 0, \"American\" : 0}\n","\n","for airline in airline_names:\n","  # Extract airline series\n","  cur_airline = df[df['airline'] == airline]\n","\n","  # Find the frequency for positive, negative, and neutral\n","  counts = cur_airline['airline_sentiment'].value_counts()\n","\n","  # Append each value to our frequency dictionary\n","  neg[airline] = (counts[0])\n","  neutral[airline] = (counts[1])\n","  pos[airline] = (counts[2])\n","\n","  # Calculate total number of sentiments for each airline and calculate fraction\n","  total = counts[0] + counts[1] + counts[2]\n","  neg_frac[airline] = counts[0] / total\n","  pos_frac[airline] = counts[2] / total\n","\n","# Number of positives and negatives for each airline\n","print(\"Negative dict:\", neg)\n","print(\"Positive dict:\", pos)\n","print(\"Neutral dict:\", neutral)\n","\n","# Fraction of positives and negatives for each airline\n","print(\"\\nNegative fraction dict:\", neg_frac)\n","print(\"Positive fraction dict:\", pos_frac)\n","\n","# Sort the dictionary\n","print(\"\\nFraction with Negative sentiments in decreasing value:\")\n","sorted_name_neg = sorted(neg_frac, key=neg_frac.get, reverse = True)\n","for name in sorted_name_neg:\n","    print(name, neg_frac[name])\n","\n","print(\"\\nFraction with positive sentiments in decreasing value:\")\n","sorted_name_neg = sorted(pos_frac, key=pos_frac.get, reverse = True)\n","for name in sorted_name_neg:\n","    print(name, pos_frac[name])"],"execution_count":163,"outputs":[{"output_type":"stream","text":["Negative dict: {'Virgin America': 181, 'United': 2633, 'Southwest': 1186, 'Delta': 955, 'US Airways': 2263, 'American': 1960}\n","Positive dict: {'Virgin America': 152, 'United': 492, 'Southwest': 570, 'Delta': 544, 'US Airways': 269, 'American': 336}\n","Neutral dict: {'Virgin America': 171, 'United': 697, 'Southwest': 664, 'Delta': 723, 'US Airways': 381, 'American': 463}\n","\n","Negative fraction dict: {'Virgin America': 0.35912698412698413, 'United': 0.6889063317634746, 'Southwest': 0.4900826446280992, 'Delta': 0.4297929792979298, 'US Airways': 0.7768623412289736, 'American': 0.7104023196810438}\n","Positive fraction dict: {'Virgin America': 0.30158730158730157, 'United': 0.12872841444270017, 'Southwest': 0.23553719008264462, 'Delta': 0.2448244824482448, 'US Airways': 0.09234466186062479, 'American': 0.12178325480246466}\n","\n","Fraction with Negative sentiments in decreasing value:\n","US Airways 0.7768623412289736\n","American 0.7104023196810438\n","United 0.6889063317634746\n","Southwest 0.4900826446280992\n","Delta 0.4297929792979298\n","Virgin America 0.35912698412698413\n","\n","Fraction with positive sentiments in decreasing value:\n","Virgin America 0.30158730158730157\n","Delta 0.2448244824482448\n","Southwest 0.23553719008264462\n","United 0.12872841444270017\n","American 0.12178325480246466\n","US Airways 0.09234466186062479\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fD26v_P4dSZV","executionInfo":{"status":"ok","timestamp":1607030397242,"user_tz":480,"elapsed":12969,"user":{"displayName":"Chia-Yuan Chang","photoUrl":"","userId":"01037799148776894861"}},"outputId":"b97ce08d-3693-4623-b5a2-034030fc2c75"},"source":["# Report\n","print(\"The top 3 airlines in terms of the fraction of negative tweets are: US Airways, American, United\")\n","print(\"The top 3 airlines in terms of the fraction of positive tweets are: Virgin America, Delta, Southwest\")"],"execution_count":164,"outputs":[{"output_type":"stream","text":["The top 3 airlines in terms of the fraction of negative tweets are: US Airways, American, United\n","The top 3 airlines in terms of the fraction of positive tweets are: Virgin America, Delta, Southwest\n"],"name":"stdout"}]}]}